{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and PyTorch Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch will train neural networks on cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import gym\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from typing import List, Tuple, Deque, Optional, Callable\n",
    "import time\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from tqdm.notebook import tqdm\n",
    "import collections\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from typing import List, Tuple, Deque, Optional, Callable\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# Setup PyTorch Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU\n",
    "print(f\"PyTorch will train neural networks on {device}\")\n",
    "\n",
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    run_subprocess_command(\"apt install swig xvfb x11-utils\")\n",
    "    run_subprocess_command(\"pip install gymnasium[box2d] pyvirtualdisplay cma\")\n",
    "# To display GIF images in the notebook\n",
    "\n",
    "import imageio     # To render episodes in GIF images (otherwise there would be no render on Google Colab)\n",
    "                   # C.f. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#bonus-make-a-gif-of-a-trained-agent\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "\n",
    "if is_colab():\n",
    "    import pyvirtualdisplay\n",
    "\n",
    "    _display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                        size=(1400, 900))\n",
    "    _ = _display.start()\n",
    "\n",
    "class RenderWrapper:\n",
    "    def __init__(self, env, force_gif=False):\n",
    "        self.env = env\n",
    "        self.force_gif = force_gif\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "\n",
    "    def render(self):\n",
    "        if not is_colab():\n",
    "            self.env.render()\n",
    "            time.sleep(1./60.)\n",
    "\n",
    "        if is_colab() or self.force_gif:\n",
    "            img = self.env.render()         # Assumes env.render_mode == 'rgb_array'\n",
    "            self.images.append(img)\n",
    "\n",
    "    def make_gif(self, filename=\"render\"):\n",
    "        if is_colab() or self.force_gif:\n",
    "            imageio.mimsave(filename + '.gif', [np.array(img) for i, img in enumerate(self.images) if i%2 == 0], fps=29, loop=0)\n",
    "            return Image(open(filename + '.gif','rb').read())\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, env, force_gif=False):\n",
    "        env.render_wrapper = cls(env, force_gif=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    An Epsilon-Greedy policy.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        The initial probability of choosing a random action.\n",
    "    epsilon_min : float\n",
    "        The minimum probability of choosing a random action.\n",
    "    epsilon_decay : float\n",
    "        The decay rate for the epsilon value after each action.\n",
    "    env : gym.Env\n",
    "        The environment in which the agent is acting.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-Network used to estimate action values.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(state: np.ndarray) -> np.int64\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "    decay_epsilon()\n",
    "        Decay the epsilon value after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 epsilon_start: float,\n",
    "                 epsilon_min: float,\n",
    "                 epsilon_decay:float,\n",
    "                 env: gym.Env,\n",
    "                 q_network: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of EpsilonGreedy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon_start : float\n",
    "            The initial probability of choosing a random action.\n",
    "        epsilon_min : float\n",
    "            The minimum probability of choosing a random action.\n",
    "        epsilon_decay : float\n",
    "            The decay rate for the epsilon value after each episode.\n",
    "        env : gym.Env\n",
    "            The environment in which the agent is acting.\n",
    "        q_network : torch.nn.Module\n",
    "            The Q-Network used to estimate action values.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "\n",
    "    def __call__(self, state: np.ndarray):\n",
    "        \"\"\"\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "\n",
    "        If a randomly chosen number is less than epsilon, a random action is chosen.\n",
    "        Otherwise, the action with the highest estimated action value is chosen.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The current state of the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.int64\n",
    "            The chosen action.\n",
    "        \"\"\"\n",
    "\n",
    "        random_float = np.random.uniform(0., 1.)\n",
    "        if (random_float < self.epsilon):\n",
    "            action = np.random.uniform(-1, 1, size=(2,))\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            action = self.q_network(state_tensor).cpu().detach().numpy()\n",
    "\n",
    "        print('Action: ', action)\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the epsilon value after each episode.\n",
    "\n",
    "        The new epsilon value is the maximum of `epsilon_min` and the product of the current\n",
    "        epsilon value and `epsilon_decay`.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
    "    def __init__(self, optimizer: torch.optim.Optimizer, lr_decay: float, last_epoch: int = -1, min_lr: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of MinimumExponentialLR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            The optimizer whose learning rate should be scheduled.\n",
    "        lr_decay : float\n",
    "            The multiplicative factor of learning rate decay.\n",
    "        last_epoch : int, optional\n",
    "            The index of the last epoch. Default is -1.\n",
    "        min_lr : float, optional\n",
    "            The minimum learning rate. Default is 1e-6.\n",
    "        \"\"\"\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            The learning rates of each parameter group.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            max(base_lr * self.gamma ** self.last_epoch, self.min_lr)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A Replay Buffer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : collections.deque\n",
    "        A double-ended queue where the transitions are stored.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    add(state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool)\n",
    "        Add a new transition to the buffer.\n",
    "    sample(batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "        Sample a batch of transitions from the buffer.\n",
    "    __len__()\n",
    "        Return the current size of the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes a ReplayBuffer instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of transitions that can be stored in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool):\n",
    "        \"\"\"\n",
    "        Add a new transition to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The state vector of the added transition.\n",
    "        action : np.int64\n",
    "            The action of the added transition.\n",
    "        reward : float\n",
    "            The reward of the added transition.\n",
    "        next_state : np.ndarray\n",
    "            The next state vector of the added transition.\n",
    "        done : bool\n",
    "            The final state of the added transition.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "            A batch of `batch_size` transitions.\n",
    "        \"\"\"\n",
    "        # Here, `random.sample(self.buffer, batch_size)`\n",
    "        # returns a list of tuples `(state, action, reward, next_state, done)`\n",
    "        # where:\n",
    "        # - `state`  and `next_state` are numpy arrays\n",
    "        # - `action` and `reward` are floats\n",
    "        # - `done` is a boolean\n",
    "        #\n",
    "        # `states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))`\n",
    "        # generates 5 tuples `state`, `action`, `reward`, `next_state` and `done`, each having `batch_size` elements.\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Q-Network implemented with PyTorch.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        First fully connected layer.\n",
    "    layer2 : torch.nn.Linear\n",
    "        Second fully connected layer.\n",
    "    layer3 : torch.nn.Linear\n",
    "        Third fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the QNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        nn_l1 : int\n",
    "            The number of neurons on the first layer.\n",
    "        nn_l2 : int\n",
    "            The number of neurons on the second layer.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # TODO...\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(n_observations, nn_l1)\n",
    "        self.layer2 = torch.nn.Linear(nn_l1, nn_l2)\n",
    "        self.layer3 = torch.nn.Linear(nn_l2, n_actions)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the forward pass of the QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor (state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (Q-values).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO...\n",
    "        x = x.to(device)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size is: 4\n",
      "Actions are: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=False, render_mode=\"rgb_array\")\n",
    "action_dim = env.action_space.n.item()\n",
    "print(f\"Action space size is: { action_dim }\")\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "q_network = QNetwork(state_dim, action_dim, 32, 16).to(device)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(150):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    # action = np.array([-1., 0.])\n",
    "    # action = np.array([0., 0.])\n",
    "    # action = env.action_space.sample()   # Random policy\n",
    "    \n",
    "    observation_tensor = torch.tensor(observation, device=device)\n",
    "    action = q_network(observation_tensor).cpu().detach().numpy()\n",
    "\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab7_ex4_explore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_agent(env: gym.Env,\n",
    "                    q_network: torch.nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    loss_fn: Callable,\n",
    "                    epsilon_greedy: EpsilonGreedy,\n",
    "                    device: torch.device,\n",
    "                    lr_scheduler: _LRScheduler,\n",
    "                    num_episodes: int,\n",
    "                    gamma: float,\n",
    "                    batch_size: int,\n",
    "                    replay_buffer: ReplayBuffer) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # Get epislon greedy action\n",
    "            action = epsilon_greedy(state)\n",
    "            # Step in environment\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            # Store experiences\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "            # Sample experiences\n",
    "            if (len(replay_buffer) > batch_size):\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                loss = 0\n",
    "                # Compute loss\n",
    "                for batch_id in range(batch_size):\n",
    "                    batch_state = states[batch_id]\n",
    "                    batch_reward = rewards[batch_id]\n",
    "                    batch_next_state = next_states[batch_id]\n",
    "                    batch_done = dones[batch_id]\n",
    "                    # Set target\n",
    "                    if (not batch_done):\n",
    "                        batch_next_state_tensor = torch.tensor(batch_next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                        y = batch_reward + gamma * q_network(batch_next_state_tensor)\n",
    "                    else:\n",
    "                        y = torch.ones(2, device = device) * batch_reward\n",
    "                    # Set estimate\n",
    "                    batch_state_tensor = torch.tensor(batch_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                    y_hat = q_network(batch_state_tensor)\n",
    "                    # Compute loss\n",
    "                    loss += loss_fn(y, y_hat)\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "    \n",
    "            # Accumulate rewards\n",
    "            episode_reward += reward\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            # Check if done\n",
    "            if (done):\n",
    "                break\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# NUMBER_OF_TRAININGS = 20\n",
    "dqn1_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(1):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn1_agent(env,\n",
    "                                           q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=10,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer)\n",
    "    dqn1_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn1_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn1_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn1_trains_result_df = pd.DataFrame(np.array(dqn1_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn1_trains_result_df[\"agent\"] = \"DQN 2013\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, \"dqn1_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dqn1_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([dqn1_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
